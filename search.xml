<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[神经网络理论介绍及实现]]></title>
    <url>%2Fp%2F3045050681%2F</url>
    <content type="text"><![CDATA[神经网络架构假设我们的神经网络结构如下图所示，每一个圆都代表一个神经元，第一层被称为输入层，最后一层被称为输出层，位于中间的被称为隐藏层。输入层和输出层的设计往往是非常直接的。以我们需要学习的MNIST数据集为例，想要验证一张手写的数字图片是否为9，假设图片大小为$64\times64$，那么就有$64\times64$个输入神经元，输出层则只有一个神经元，当输出值大于$0.5$时表明输入的图片是9，输出值小于$0.5$时，表明输入的图片不是9。 反向传播算法与回归问题一样，我们也需要通过最小化代价函数来优化预测精度，但是由于神经网络包含了多个隐藏层，每个隐藏层都会输出预测，因此无法通过传统的梯度下降方法来最小化代价函数，而需要逐层考虑误差，并逐层优化。因此，在多层神经网络里面，我们需要通过反向传播算法优化预测精度。 算法流程在实际应用中，我们一般将反向传播算法与学习算法一起使用，例如Stochatic Gradiant Decent。结合之后的算法流程总结如下： 输入$n$个训练样本 对于每一个训练样本$x_i, i \in {1,2,\cdots,n}$：设置输入层的对应激活值为$a_i^1$，然后执行以下步骤： 前向传播： 对于$l \in {2, 3, \cdots, L}$，分别计算$z_i^l = w^la_i^{l-1}+b^l$，$a_i^l = \sigma(z_i^l)$。 输出层误差$\delta^L$：计算$\delta_i^L = \nabla_aC.*\sigma’(z_i^L)$ 反向传播误差：对于$l \in {L-1, L-2, \cdots, 2}$，分别计算$\delta_i^l = ((w^{l+1})^T\delta_i^{l+1}).*\sigma’(z_i^l)$。 梯度下降:对于$l \in {L, L-1, \cdots, 2}$，更新$w^l \rightarrow w^l-\frac{\alpha}{n}\sum_i\delta_i^l(a^{l-1}_i)^T$，$b^l \rightarrow b^l-\frac{\alpha}{n}\sum_i\delta_i^l$。 理论推导我们的最终目标是计算$\min_{w,b}C(w, b)$，即找到一组参数$(w,b)$使得代价函数$C$最小。因此我们需要计算$\frac{\partial C}{\partial w}$和$\frac{\partial C}{\partial b}$，从而结合梯度下降算法求得$C$的最小值。接下来将以计算$\frac{\partial C}{\partial w}$为例进行说明。 计算输出层$L$的偏导$\frac{\partial C}{\partial w^L}$。根据链式法则，我们可以得到下式： $$ \frac{\partial C}{\partial w^L} = \bbox[yellow,5px,border:2px solid red]{\frac{\partial C}{\partial a^L} \frac{\partial a^L}{\partial z^L}} \frac{\partial z^L}{\partial w^L} \tag{1}. $$ 计算隐藏层$L-1$的偏导$\frac{\partial C}{\partial w^{L-1}}$。根据链式法则，我们可以得到下式： $$ \frac{\partial C}{\partial w^{L-1}} = \bbox[yellow,5px,border:2px solid red] {\frac{\partial C}{\partial a^L} \frac{\partial a^L}{\partial z^L}} \frac{\partial z^L}{\partial a^{L-1}}\frac{\partial a^{L-1}}{\partial z^{L-1}} \frac{\partial z^{L-1}}{\partial w^{L-1}} \tag{2}. $$ 观察公式$(1),(2)$，很明显用红框圈出来的是两个式子共有的一部分，通常我们称之为$\delta^{L}$，表达式如公式$(3)$所示。我们可以用$\delta^{L}$来计算输出层前一层的偏导。$$\delta^L = \frac{\partial C}{\partial a^L} \frac{\partial a^L}{\partial z^L} \tag{3}.$$ 同理，隐藏层之间也有类似的共有部分，例如我们可以用$\delta^{L-1}$来计算隐藏层最后一层的前一层的偏导，表达式如公式$(4)$所示。$$\begin{align}\delta^{L-1} &amp;= \frac{\partial C}{\partial a^L} \frac{\partial a^L}{\partial z^L} \frac{\partial z^L}{\partial a^{L-1}}\frac{\partial a^{L-1}}{\partial z^{L-1}} \\&amp;= \delta^L \frac{\partial z^L}{\partial a^{L-1}} \frac{\partial a^{L-1}}{\partial z^{L-1}}.\end{align} \tag{4}$$ 通过公式$(3),(4)$，公式$(1),(2)$可以改写为以下形式：$$\begin{eqnarray}\frac{\partial C}{\partial w^L} &amp;=&amp; \delta^L \frac{\partial z^L}{\partial w^L} \tag{5}, \\\frac{\partial C}{\partial w^{L-1}} &amp;=&amp; \delta^{L-1} \frac{\partial z^{L-1}}{\partial w^{L-1}} \tag{6}.\end{eqnarray}$$ 假设激活函数为$\sigma(z)$，对公式$(3)\sim(6)$进行详细的计算。$$\begin{eqnarray}\delta^L &amp;=&amp; \frac{\partial C}{\partial a^L} \frac{\partial a^L}{\partial z^L} \\&amp;=&amp; \nabla_aC \cdot\sigma’(z^L) \tag{7}, \\\\\delta^{L-1} &amp;=&amp; \delta^L \frac{\partial z^L}{\partial a^{L-1}} \frac{\partial a^{L-1}}{\partial z^{L-1}} \\&amp;=&amp; \delta^L \frac{\partial (w^La^{L-1}+b^L)}{\partial a^{L-1}} \sigma’(z^{L-1}) \\&amp;=&amp; \delta^Lw^L\sigma’(z^{L-1}) \tag{8}, \\\\\frac{\partial C}{\partial w^L} &amp;=&amp; \delta^L \frac{\partial z^L}{\partial w^L} \\&amp;=&amp; \delta^L \frac{\partial (w^La^{L-1}+b^L)}{\partial w^L} \\&amp;=&amp; \delta^La^{L-1} \tag{9}, \\\\\frac{\partial C}{\partial w^{L-1}} &amp;=&amp; \delta^{L-1} \frac{\partial z^{L-1}}{\partial w^{L-1}} \\&amp;=&amp; \delta^{L-1} \frac{\partial (w^{L-1}a^{L-2}+b^{L-1})}{\partial w^{L-1}} \\&amp;=&amp; \delta^{L-1}a^{L-2} \tag{10}.\end{eqnarray}$$ 按照相同的原理，我们可以推得：$$\begin{align}\frac{\partial C}{\partial b^L} &amp;= \delta^L \frac{\partial z^L}{\partial b^L} \\&amp;= \delta^L \frac{\partial (w^La^{L-1}+b^L)}{\partial b^L} \\&amp;= \delta^L \tag{11}.\end{align}$$ 将公式$(9),(10)$合并，并用$l, l+1$分别替换$L-1, L$，则公式$(7)\sim(11)$可总结为以下三个式子：$$\begin{align}\delta^l = \begin{cases} \nabla_aC\cdot\sigma’(z^L), &amp; \qquad \text{if $l = L$}, \\ \delta^{l+1}w^{l+1}\sigma’(z^{l}), &amp; \qquad \text{if $l\in{L-1, L-2, \cdots, 2}$}, \end{cases} \tag{12} \\\frac{\partial C}{\partial w^l} = \delta^la^{l-1}, \qquad l \in {L, L-1, \cdots, 2} \tag{13}, \\\frac{\partial C}{\partial b^l} = \delta^l, \qquad l \in {L, L-1, \cdots, 2} \tag{14}.\end{align}$$ 观察公式$(12)\sim(14)$可以看出，当前层的代价函数偏导，需要依赖于后一层的计算结果。这也是为什么这个算法的名称叫做反向传播算法。 应用实践接下来我们将用反向传播算法对MNIST手写数字数据集进行识别。这个问题比较简单，数字共有10种可能，分别为${0, 1, \cdots, 9}$，因此是一个10分类问题。 完整代码请参考GitHub: machine-learning-notes(python3.6) 载入数据首先我们从MNIST手写数字数据集官网下载训练集和测试集，并解压到data文件夹中，data文件夹中应该包含t10k-images.idx3-ubyte, t10k-labels.idx1-ubyte, train-images.idx3-ubyte, train-labels.idx1-ubyte这四个文件。接下来通过python-mnist包对数据集进行导入。如果尚未安装该包，可通过以下命令进行安装： 1pip install python-mnist 使用python-mnist包载入数据，代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npfrom mnist import MNISTfrom sklearn.preprocessing import MinMaxScalerdef vectorized_result(j): """ 将数字(0...9)变为one hot向量 输入： j: int，数字(0...9) 输出： e: np.ndarray, 10维的向量，其中第j位为1，其他位都为0。 """ e = np.zeros((10, 1)); e[j] = 1.0 return edef load_data_wrapper(dirpath): """ 载入mnist数字识别数据集，并对其进行归一化处理 输入： dirpath: str, 数据所在文件夹路径 输出： training_data: list, 包含了60000个训练数据集，其中每一个数据由一个tuple '(x, y)'组成， x是训练的数字图像，类型是np.ndarray, 维度是(784,1) y表示训练的图像所属的标签，是一个10维的one hot向量 test_data: list, 包含了10000个测试数据集，其中每一个数据由一个tuple '(x, y)'组成， x是测试的数字图像，类型是np.ndarray, 维度是(784,1) y表示测试的图像所属标签，int类型，是一个(0...9)的数字 """ mndata = MNIST(dirpath) tr_i, tr_o = mndata.load_training() te_i, te_o = mndata.load_testing() min_max_scaler = MinMaxScaler() tr_i = min_max_scaler.fit_transform(tr_i) te_i = min_max_scaler.transform(te_i) training_inputs = [np.reshape(x, (784, 1)) for x in tr_i] training_outputs = [vectorized_result(y) for y in tr_o] training_data = list(zip(training_inputs, training_outputs)) test_inputs = [np.reshape(x, (784, 1)) for x in te_i] test_data = list(zip(test_inputs, te_o)) return training_data, test_datatraining_data, test_data = load_data_wrapper("../data/") 执行时，你可能会遇到下面的错误： 1FileNotFoundError: [Errno 2] No such file or directory: '../data/t10k-images-idx3-ubyte' 这是因为python-mnist包中批量载入数据集时默认的文件名为t10k-images-idx3-ubyte，而从官网下载的数据集文件名为t10k-images.idx3-ubyte，因此只需要修改data文件夹中的文件名即可成功运行。 构建神经网络 网络初始化 搭建网络的基本框架，包括神经网络各个层的数目，以及初始化参数。 12345678910111213class Network(object): def __init__(self, sizes): """初始化神经网络 1. 根据输入，得到神经网络的结构 2. 根据神经网络的结构使用均值为0，方差为1的高斯分布初始化参数权值w和偏差b。 输入： sizes: list, 表示神经网络各个layer的数目，例如[784, 30, 10]表示3层的神经网络。 输入层784个神经元，隐藏层只有1层，有30个神经元，输出层有10个神经元。 """ self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] 随机梯度下降 123456789101112131415161718192021222324def SGD(self, training_data, epochs, mini_batch_size, alpha, test_data=None): """随机梯度下降 输入： training_data：是由tuples ``(x, y)``组成的list，x表示输入，y表示预计输出 epoches：int, 表示训练整个数据集的次数 mini_batch_size: int, 在SGD过程中每次迭代使用训练集的数目 alpha: float, 学习速率 test_data: 是由tuples ``(x, y)``组成的list，x表示输入，y表示预计输出。 如果提供了``test_data``，则每经过一次epoch，都计算并输出当前网络训练结果在测试集上的准确率。 虽然可以检测网络训练效果，但是会降低网络训练的速度。 """ if test_data: n_test = len(test_data) m = len(training_data) for j in range(epochs): np.random.shuffle(training_data) mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, m, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, alpha) if test_data: print("Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;".format(j, self.evaluate(test_data), n_test)) else: print("Epoch &#123;0&#125; complete".format(j)) 更新权值$w$和偏差$b$ 12345678910111213141516def update_mini_batch(self, mini_batch, alpha): """每迭代一次mini_batch，根据梯度下降方法，使用反向传播得到的结果更新权值``w``和偏差``b`` 输入： mini_batch: 由tuples ``(x, y)``组成的list alpha: int，学习速率 """ nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nable_w = self.back_prop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nable_w)] self.weights = [w-(alpha/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(alpha/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] 反向传播 123456789101112131415161718192021222324252627282930313233343536def back_prop(self, x, y): """反向传播 1. 前向传播，获得每一层的激活值 2. 根据输出值计算得到输出层的误差``delta`` 3. 根据``delta``计算输出层C_x对参数``w``, ``b``的偏导 4. 反向传播得到每一层的误差，并根据误差计算当前层C_x对参数``w``, ``b``的偏导 输入： x: np.ndarray, 单个训练数据 y: np.ndarray, 训练数据对应的预计输出值 输出： nabla_b: list, C_x对``b``的偏导 nabla_w: list, C_x对``w``的偏导 """ nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # forward prop activation = x activations = [x] zs = [] for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward prop delta = self.cost_derivative(activations[-1], y)*sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) for l in range(2, self.num_layers): z = zs[-l]; sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta)*sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) $C_x$对$a^L$的偏导 123456789def cost_derivative(self, output_activations, y): """代价函数对a的偏导 输入： output_activations： np.ndarray, 输出层的激活值，即a^L y: np.ndarray, 预计输出值 输出： output_activations-y: list, 偏导值 """ return (output_activations-y) 准确率计算 12345678910def evaluate(self, test_data): """计算准确率，将测试集中的x带入训练后的网络计算得到输出值， 并得到最终的分类结果，与预期的结果进行比对，最终得到测试集中被正确分类的数目 输入： test_data: 由tuples ``(x, y)``组成的list 输出： int, 测试集中正确分类的数据个数 """ test_results = [(np.argmax(self.feed_forward(x)), y) for x, y in test_data] return sum(int(x==y) for (x, y) in test_results) 前馈根据当前网络训练的结果，对数据$x$进行预测 12345678910def feed_forward(self, a): """前馈 输入： a：np.ndarray 输出： a：np.ndarray，预测输出 """ for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a 激活函数及其导数 1234567def sigmoid(z): """The sigmoid function""" return 1.0/(1.0+np.exp(-z))def sigmoid_prime(z): """Derivative of the sigmoid function""" return sigmoid(z)*(1-sigmoid(z)) 训练 训练数据时使用的电脑型号为MacBook Pro (13-inch, Early 2015)，时间仅供参考 训练全部的数据需要一定的时间（用时$3min53s$），如果想要快速的查看训练结果，可以取部分训练集和测试集进行训练和测试。 12net = Network([784, 30, 10])net.SGD(training_data, 30, 10, 3.0, test_data=test_data) 输出： Epoch 0: 9121 / 10000 Epoch 1: 9222 / 10000 Epoch 2: 9302 / 10000 Epoch 3: 9369 / 10000 Epoch 4: 9356 / 10000 ...... Epoch 25: 9503 / 10000 Epoch 26: 9508 / 10000 Epoch 27: 9513 / 10000 Epoch 28: 9508 / 10000 Epoch 29: 9529 / 10000 可以看到经过30轮的训练，准确率已经达到了$95.29\%$（epoch 29）。作为第一次尝试，这个准确率已经非常令人满意了。 接下来我们增大隐藏层的层数，例如50，来重新训练，看看效果如何。隐藏层增加后，训练速度会变得更加缓慢（用时$5min15s$），在等待训练完成的过程中，可以去倒杯茶，放松一下身体。 12net = Network([784, 50, 10])net.SGD(training_data, 30, 10, 3.0, test_data=test_data) 输出： Epoch 0: 9176 / 10000 Epoch 1: 9307 / 10000 Epoch 2: 9416 / 10000 Epoch 3: 9441 / 10000 Epoch 4: 9480 / 10000 ...... Epoch 25: 9584 / 10000 Epoch 26: 9602 / 10000 Epoch 27: 9581 / 10000 Epoch 28: 9582 / 10000 Epoch 29: 9599 / 10000 观察结果可以发现准确率上升到了$96.02\%$（epoch 26）。增加隐藏层的确提高了训练的准确率。但是并非一直如此。接下来我们继续增加隐藏层数目，将其设置为100，并设置epoches=100，发现准确率变为了$87.84\%$，较之前反而下降了，并且训练的时间也延长到了$27min 15s$。因此在设置隐藏层数目时，不能盲目的增加隐藏层数目，否则只会费力不讨好，既降低了准确率，又增加了训练所需时间。比较好的办法是先根据经验设置一个初始值，然后在初始值的基础上慢慢增加，从而得到一个合理的数字。 12net = Network([784, 50, 10])net.SGD(training_data, 30, 10, 3, test_data=test_data) 输出： Epoch 0: 7308 / 10000 Epoch 1: 7572 / 10000 Epoch 2: 7642 / 10000 Epoch 3: 8604 / 10000 Epoch 4: 8644 / 10000 Epoch 5: 8655 / 10000 Epoch 6: 8665 / 10000 ...... Epoch 54: 8772 / 10000 Epoch 55: 8776 / 10000 Epoch 56: 8782 / 10000 Epoch 57: 8784 / 10000 Epoch 58: 8779 / 10000 Epoch 59: 8777 / 10000 参考文献Michael A. Nielsen, “Neural network and deep learning”, Determination Press, 2015]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>neural network</tag>
        <tag>backward propagation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[继承与组合]]></title>
    <url>%2Fp%2F351146056%2F</url>
    <content type="text"><![CDATA[问题引入现在需要提供这样一个功能： 记录HashSet自从创建以来总共添加的元素个数 为了提供这种功能，直觉上的做法就是继承HashSet，并增加一个变量，用它记录插入的元素数量，并针对该计数值导出一个访问方法。HashSet类包含两个可以增加元素的方法： add， addAll，因此需要覆盖这两个方法。 照此思路可得代码如下： 123456789101112131415161718192021222324public class InstrumentedHashSet&lt;E&gt; extends HashSet&lt;E&gt; &#123; // the number of attempted element insertions private int addCount = 0; public InstrumentedHashSet() &#123; &#125; @Override public boolean add(E e) &#123; addCount++; return super.add(e); &#125; @Override public boolean addAll(Collection&lt;? extends E&gt; c) &#123; addCount += c.size(); return super.addAll(c); &#125; public int getAddCount() &#123; return addCount; &#125;&#125; 为了测试其功能，编写以下测试代码： 123456789101112131415161718192021222324252627public class InstrumentedHashSetTest &#123; private InstrumentedHashSet&lt;String&gt; set; @Before public void setUp() throws Exception &#123; set = new InstrumentedHashSet&lt;&gt;(); &#125; @Test public void add() throws Exception &#123; String s = "Snap"; set.add(s); Assert.assertThat(set, hasItem(s)); &#125; @Test public void addAll() throws Exception &#123; String[] s = &#123;"Snap", "Crackle", "Pop"&#125;; set.addAll(Arrays.asList(s)); Assert.assertThat(set, hasItems(s)); &#125; @Test public void getAddCount() throws Exception &#123; addAll(); Assert.assertEquals(set.getAddCount(), 3); &#125;&#125; 运行结果如下： getAddCount(effectivejava.InstrumentedHashSetTest): expected: but was:false 期望的返回值是3，结果返回的是6。原因在哪？ 看HashSet中add和addAll的实现源码： 1234567891011public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125;public boolean addAll(Collection&lt;? extends E&gt; c) &#123; boolean modified = false; for (E e : c) if (add(e)) modified = true; return modified;&#125; 可以看到addAll方法的实现是循环调用add方法实现的，因此继承了该方法之后，计数值在调用addCount时累加了3，在循环调用add方法时又累加了3，总计增加了6。通过addAll方法增加的每个元素都被计算了两次。这显然跟功能需求不符。 通过上述例子可以看到，继承虽然是实现代码重用的有力手段，但是并非永远是最佳的工具。继承的适用场景和缺点总结如下： 适用场景 包的内部适用。在那里，子类和超类的实现都处在同一个程序员的控制之下。 专门为了继承而设计，并具有良好的文档说明的类。 缺点 继承打破了封装性。如果子类依赖于其超类中特定功能的实现细节。当超类发生了变更，那么子类可能会遭到破坏。上述实例也表明了这一点。 组合上述问题可以通过组合（composition）来解决。组合就是不扩展现有的类，而是在新的类中增加一个私有域，它引用现有类的一个实例。现有的类变成了新类的一个组件。新类中的每个实例方法都可以调用被包含的现有类的实例中对应的方法，并返回它的结果。这被称为转发（forwarding），新类中的方法被称为转发方法（forwarding method）。 接下来用组合和转发的方法实现上述功能。由于Set接口保存了HashSet类的功能特性，因此可以设计一个转发类实现Set接口，并且拥有单个构造器，参数为Set类型。这个包装类就可以用来包装任何Set实现，并且可以结合任何先前存在的构造器一起工作。照此思路可得转发类代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class ForwardingSet&lt;E&gt; implements Set&lt;E&gt; &#123; private final Set&lt;E&gt; s; public ForwardingSet(Set&lt;E&gt; s) &#123; this.s = s; &#125; @Override public int size() &#123; return s.size(); &#125; @Override public boolean isEmpty() &#123; return s.isEmpty(); &#125; @Override public boolean contains(Object o) &#123; return s.contains(o); &#125; @Override public Iterator&lt;E&gt; iterator() &#123; return s.iterator(); &#125; @Override public Object[] toArray() &#123; return s.toArray(); &#125; @Override public &lt;T&gt; T[] toArray(T[] a) &#123; return s.toArray(a); &#125; @Override public boolean add(E e) &#123; return s.add(e); &#125; @Override public boolean remove(Object o) &#123; return s.remove(o); &#125; @Override public boolean containsAll(Collection&lt;?&gt; c) &#123; return s.containsAll(c); &#125; @Override public boolean addAll(Collection&lt;? extends E&gt; c) &#123; return s.addAll(c); &#125; @Override public boolean retainAll(Collection&lt;?&gt; c) &#123; return s.retainAll(c); &#125; @Override public boolean removeAll(Collection&lt;?&gt; c) &#123; return s.removeAll(c); &#125; @Override public void clear() &#123; s.clear(); &#125;&#125; 由于每个InstrumentedSet实例都被另一个Set实例包装起来了，因此InstrumentedSet被称为包装类（wrapper class）。这也正是装饰（Decorator）模式。包装类代码如下： 1234567891011121314151617181920212223public class InstrumentedSet&lt;E&gt; extends ForwardingSet&lt;E&gt; &#123; private int addCount = 0; public InstrumentedSet(Set&lt;E&gt; s) &#123; super(s); &#125; @Override public boolean add(E e) &#123; addCount++; return super.add(e); &#125; @Override public boolean addAll(Collection&lt;? extends E&gt; c) &#123; addCount += c.size(); return super.addAll(c); &#125; public int getAddCount() &#123; return addCount; &#125;&#125; 编写测试代码如下： 1234567891011121314151617181920212223242526272829@FixMethodOrder(MethodSorters.NAME_ASCENDING)public class InstrumentedSetTest &#123; private InstrumentedSet&lt;String&gt; set; @Before public void setUp() throws Exception &#123; set = new InstrumentedSet&lt;&gt;(new TreeSet&lt;String&gt;()); &#125; @Test public void add() throws Exception &#123; String s = "Snap"; set.add(s); Assert.assertThat(set, hasItem(s)); &#125; @Test public void addAll() throws Exception &#123; String[] s = &#123;"Snap", "Crackle", "Pop"&#125;; set.addAll(Arrays.asList(s)); Assert.assertThat(set, hasItems(s)); &#125; @Test public void getAddCount() throws Exception &#123; addAll(); Assert.assertEquals(3, set.getAddCount()); &#125;&#125; 测试通过。 总结只有当子类真正是超类的子类型时，才适合用继承。也就是说只有当两个类A,B之间确实存在“is-a”关系时，才可使用继承。如果不能完全确定这个关系时，通常情况下B应该包含A的一个私有实例，并且暴露一个较小的、较为简单的API；A本质上不是B的一部分，只是它的实现细节而已。如果不能确保是继承的使用场景，那么为了避免导致脆弱性，可以用组合和转发来代替继承。尤其是存在适当的接口可以实现包装类的时候。包装类不仅比子类更为健壮，而且功能也更强大。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[覆盖equals和hashcode方法]]></title>
    <url>%2Fp%2F1563918912%2F</url>
    <content type="text"><![CDATA[覆盖equals方法覆盖条件当类具有自己特有的“逻辑相等”概念时（与对象等同的概念不相同），这时就需要覆盖equals方法。 通用约定查看Object类源码的equals方法，注释上明确的写明了需要遵守的约定如下： 自反性：对于任何非null的引用值x， x.equals(x)将会返回true。 对称性：对于任何非null的引用值x和y，当且仅当y.equals(x)返回true时，x.equals(y)也必须返回true。 传递性：对于任何非null的引用值x、y和z，如果x.equals(y)返回true，并且y.equals(z)也返回true，那么x.equals(z)也必须返回true。 一致性：对于任何非null的引用值x和y，如果equals的比较操作中对象提供的信息没有改变，那么多次调用x.equals(y)就会一致的返回true或false。 原注释如下： It is reflexive: for any non-null reference value x.equals(x) should return true. It is symmetric: for any non-null reference values xand y, x.equals(y) should return true if and only if y.equals(x)returns true. It is transitive: for any non-null reference values x, y, and z, ifx.equals(y)returns trueand y.equals(z)returns true, then x.equals(z)should return true. It is consistent: for any non-null reference values xand y, multiple invocations of x.equals(y)consistently return true or consistently return false, provided no information used in equalscomparisons on the objects is modified. For any non-null reference value x, x.equals(null)should return false. 覆盖方法例如创建一个Employee对象如下： 1234567public class Employee &#123; private long id; private String firstName; private String lastName; ... // setter and getter method&#125; 假如需要比较两个employee，如果使用默认的equals方法如下： 123456789public class test &#123; public static void main(String[] args) &#123; Employee e1 = new Employee(); Employee e2 = new Employee(); e1.setId(1000); e2.setId(1000); System.out.print(e1.equals(e2)); &#125;&#125; 显然输出结果为false。这时候就满足了我们之前提到的覆盖条件：Employee类需要根据自身属性来判断两个类是否相同。因此可以覆盖equals方法如下： 12345678910@Overridepublic boolean equals(Object o) &#123; if (this == o) return true; if (!(o instanceof Employee)) return false; Employee employee = (Employee) o; return id == employee.id &amp;&amp; (getFirstName() != null ? getFirstName().equals(employee.getFirstName()) : employee.getFirstName() == null &amp;&amp; (getLastName() != null ? getLastName().equals(employee.getLastName()) : employee.getLastName() == null));&#125; 现在再进行测试，即可得到结果为true。 覆盖hashcode方法覆盖条件当覆盖了equals方法后，也必须覆盖hashcode方法。如果不覆盖hashcode方法，就会违反Object.hashcode的通用约定，就会导致该类无法结合所有基于散列的集合一起运作，例如Hashmap，Hashset，Hashtable等。 通用约定在Object类的源码中，对hashcode做了如下约定： 在Java应用程序执行的过程中，无论对同一个对象调用多少次，只要对象的equals方法的比较操作所用到的信息没有被修改，那么该对象的hashcode方法都必须始终返回同一个整数。在同一个应用程序的多次执行过程中，每次执行所返回的整数可以不一致。 如果两个对象通过equals方法比较是相等的，那么分别调用这两个对象的hashcode方法返回的整数必须是相等的。 如果两个对象通过equals方法比较是不等的。分别调用这两个对象的hashcode方法返回的整数不一定要相等。但是程序员应该要意识到给每个不相等的对象产生不同的整数结果可能提高散列表（hash table）的性能。 原文如下： Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent from one execution of an application to another execution of the same application. If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result. It is not required that if two objects are unequal according to the java.lang.Object#equals(java.lang.Object) method, then calling the hashCode method on each of the two objects must produce distinct integer results. However, the programmer should be aware that producing distinct integer results for unequal objects may improve the performance of hash tables. 覆盖方法同样使用Employee类来举例，考虑以下代码： 1234567891011121314public class test &#123; public static void main(String[] args) &#123; Employee e1 = new Employee(); Employee e2 = new Employee(); e1.setId(100); e2.setId(100); // print true System.out.println(e1.equals(e2)); Map&lt;Employee, String&gt; map = new HashMap&lt;&gt;(); map.put(e1, "Jenny"); // print null System.out.println(map.get(e2)); &#125;&#125; 如何两个对象通过equals返回true，那么打印的值应该为Jenny才对。问题就出现在没有重写hashcode方法上。这就是覆盖条件中提到的覆盖了equals方法而没有覆盖hashcode导致的。那么如何覆盖hashcode呢？可简单的通过以下步骤实现： 把某个非零的常数值，比如17保存在一个名为result的int类型的变量中。 对对象中每个关键域（指equals方法中涉及的每个域），完成以下步骤： 为该域计算int类型的散列码c: 若该域是boolean类型，则计算(f?1:0)。 若该域是byte、char、short或int类型，则计算(int) f。 若该域是long类型，则计算(int) (f ^ (f &gt;&gt;&gt; 32))。 若该域是float类型，则计算Float.floatToIntBits(f)。 若该域是double类型，则计算Double.doubleToLongBits(f)，然后按照步骤2.1.3，为得到的long类型值计算散列值。 若该域是一个对象引用，并且该类的equals方法通过递归的调用equals的方式来比较这个域，则同样为这个域递归的调用hashcode。如果需要更复杂的比较，则为这个域计算一个“范式”，然后针对这个范式调用hashcode。如果这个域的值为null，则返回0（或某个其他常数，通常为0）。 若该域是一个数组，则要把每个元素当做单独的域来处理。也就是说，递归地应用上述规则，对每个重要的元素计算一个散列码，然后根据步骤2.2中的做法把这些散列值组合起来。如果数组域中的每个元素都很重要。可以利用发行版本1.5中增加的其中一个Arrays.hashcode方法。 按照下面的公式，把步骤2.1中计算得到的散列码c合并到result中： 1result = 31 * result + c; 返回result。 写完了hashcode方法后，问问自己“相等的实例是否都具有相等的散列码”。要编写单元测试来验证。若相等的实例有着不相等的散列码，则找出原因并修正。 根据上述解决方案可以得到覆盖hashcode如下： 12345678@Overridepublic int hashCode() &#123; int result = 17; result = 31 * result + (int) (id ^ (id &gt;&gt;&gt; 32)); result = 31 * result + getFirstName().hashCode(); result = 31 * result + getLastName().hashCode(); return result;&#125; 再次执行之前的测试即可成功打印Jenny。 谢谢阅读。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
